{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed035d7",
   "metadata": {},
   "source": [
    "<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->\n",
    "\n",
    "# CODE 01: ZeRO æ˜¾å­˜ä¼˜åŒ–å®è·µ\n",
    "\n",
    "> Author by: è®¸ç¿å²·\n",
    "\n",
    "ç›®å‰**GPU + Pytorch + Megatron + DeepSpeed**æ˜¯å¸¸ç”¨çš„è®­ç»ƒè¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ¡†æ¶ã€‚è€Œå¾®è½¯å¼€å‘çš„**DeepSpeed**çš„æ ¸å¿ƒå°±æ˜¯**ZeRO**(Zero Redundancy Optimizer)ï¼Œå®ƒæ˜¯ä¸€ç§æ˜¾å­˜ä¼˜åŒ–çš„**æ•°æ®å¹¶è¡Œ**(data parallelismï¼ŒDP)æ–¹æ¡ˆã€‚**ZeRO**æŠ€æœ¯é€šè¿‡æ¶ˆé™¤**æ•°æ®å¹¶è¡Œ**ä¸­çš„æ˜¾å­˜å†—ä½™ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒå¤§æ¨¡å‹æ‰€éœ€çš„æ˜¾å­˜ã€‚\n",
    "\n",
    "æœ¬å®éªŒå°†æ·±å…¥æ¢è®¨ ZeRO çš„å„çº§ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡å®é™…ä»£ç æ¼”ç¤ºå’Œåˆ†æï¼Œç†è§£ä¸åŒçº§åˆ«çš„ ZeRO å¦‚ä½•å®ç°æ˜¾å­˜ä¼˜åŒ–ã€‚\n",
    "\n",
    "ğŸ“Œ **PS**ï¼šæœ¬ Notebook **ä»…ç”¨äºæ•™å­¦ç›®çš„**ï¼Œæ‰€æœ‰ ZeRO å®ç°å‡ä¸º**å• GPU ä¸Šçš„ç®€åŒ–æ¨¡æ‹Ÿ**ï¼Œ**å¹¶æœªä½¿ç”¨çœŸå®å¤š GPU å¹¶è¡Œæˆ–é€šä¿¡åŸè¯­**ï¼ˆå¦‚ `all_reduce`, `reduce_scatter`ï¼‰ã€‚çœŸå® ZeRO éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼ˆå¦‚ DeepSpeed + å¤š GPUï¼‰ï¼Œå…¶æ˜¾å­˜èŠ‚çœæ•ˆæœåœ¨ **N ä¸ª GPU æ—¶æ‰ä½“ç°ä¸º 1/N**ã€‚æœ¬å®éªŒé€šè¿‡â€œäººä¸ºåˆ†ç‰‡ + æ‰‹åŠ¨é‡Šæ”¾â€æ¥**æ¨¡æ‹Ÿ**åˆ†ç‰‡è¡Œä¸ºï¼Œå¸®åŠ©ç†è§£æ ¸å¿ƒæ€æƒ³ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29b015c",
   "metadata": {},
   "source": [
    "\n",
    "## 1. æ¨¡å‹æ˜¾å­˜å ç”¨åˆ†æ\n",
    "\n",
    "åœ¨æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­ï¼Œæ˜¾å­˜å ç”¨å¯ä»¥åˆ†ä¸º**Activation**å’Œ**Model State**ä¸¤éƒ¨åˆ†ï¼š\n",
    "\n",
    "**Activation**ï¼š\n",
    "- **ä¸­é—´æ¿€æ´»å€¼**ï¼ˆActivationsï¼‰ï¼šåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚ä¼šäº§ç”Ÿä¸­é—´æ¿€æ´»å€¼ï¼Œè¿™äº›æ¿€æ´»å€¼éœ€è¦å†åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ç”¨æ¥è®¡ç®—æ¢¯åº¦ã€‚\n",
    "- **è¾“å…¥æ•°æ®**ï¼ˆInputs Dataï¼‰ï¼šæ‰¹å¤„ç†ä¸­è¾“å…¥æ•°æ®ä¹Ÿå ç”¨æ˜¾å­˜ï¼Œå°¤å…¶æ˜¯å½“æ‰¹å¤„ç†è¾ƒå¤§æ—¶ã€‚\n",
    "\n",
    "**Model State**ï¼š\n",
    "\n",
    "- **ä¼˜åŒ–å™¨çŠ¶æ€**ï¼ˆOptimizer Statesï¼‰ï¼šæ˜¯ Optimizer åœ¨è¿›è¡Œæ¢¯åº¦æ›´æ–°æ—¶æ‰€éœ€è¦ç”¨åˆ°æ•°æ®ã€‚ä¸€äº›ä¼˜åŒ–å™¨(å¦‚ Adam)éœ€è¦å­˜å‚¨é¢å¤–çš„çŠ¶æ€ä¿¡æ¯ï¼Œå¦‚æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡å€¼å’Œå¹³æ–¹æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡å€¼ã€‚ä¾‹å¦‚ SGD ä¸­çš„ Momentum,å³ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒæ—¶çš„**Float32 Master Parameters**ã€‚\n",
    "- **æ¨¡å‹å‚æ•°**ï¼ˆParametersï¼‰ï¼šæ¨¡å‹çš„å¯å­¦ä¹ æƒé‡ï¼Œå¦‚å­˜å‚¨åœ¨æ˜¾å­˜ä¸­çš„æ¨¡å‹æƒé‡å’Œåç½®é¡¹ã€‚\n",
    "- **æ¢¯åº¦**ï¼ˆGradientsï¼‰ï¼šåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦ï¼Œç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°ã€‚å…¶å†³å®šäº†å‚æ•°çš„æ›´æ–°æ–¹å‘ã€‚\n",
    "\n",
    "å®ƒä»¬ä¸‰ä¸ªç®€ç§°**OPG**ï¼Œå…¶ä¸­**ä¼˜åŒ–å™¨çŠ¶æ€**ä¼šå æ®å¤§çº¦ 2 å€å‚æ•°é‡çš„æ˜¾å­˜ç©ºé—´ï¼Œè¿™å–å†³äºé€‰æ‹©çš„ä¼˜åŒ–å™¨ï¼Œä¹Ÿæ˜¯æ•´ä¸ªè®­ç»ƒä¸­å æ®æœ€å¤§ç©ºé—´çš„éƒ¨åˆ†ã€‚\n",
    "\n",
    "**ZeRO**åˆ™åœ¨æ•°æ®å¹¶è¡Œçš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†å¯¹å†—ä½™**Model States**çš„ä¼˜åŒ–ã€‚ä½¿ç”¨ ZeRO åï¼Œå„ä¸ªè¿›ç¨‹ä¹‹ååªä¿å­˜å®Œæ•´çŠ¶æ€çš„**1/GPUs**ï¼Œäº’ä¸é‡å ï¼Œä¸å†å­˜åœ¨å†—ä½™ã€‚ç›¸æ¯”ä¼ ç»Ÿæ•°æ®å¹¶è¡Œçš„ç®€å•å¤åˆ¶ï¼Œ**ZeRO**é€šè¿‡å°†æ¨¡å‹çš„**å‚æ•°**ã€**æ¢¯åº¦** å’Œ **ä¼˜åŒ–å™¨çŠ¶æ€**åˆ’åˆ†åˆ°ä¸åŒçš„è¿›ç¨‹æ¥æ¶ˆé™¤å†—ä½™çš„å†…å­˜å ç”¨ï¼Œä¹Ÿå°±å¼•å‡º**ZeRO**çš„ä¸‰ä¸ªä¸åŒçš„çº§åˆ«,åˆ†åˆ«å¯¹åº”**Model States**ä¸åŒç¨‹åº¦çš„åˆ†å‰²(Partition)ï¼š\n",
    "\n",
    "- **ZeRO-1**ï¼š åˆ†å‰²**ä¼˜åŒ–å™¨çŠ¶æ€**ã€‚\n",
    "- **ZeRO-2**ï¼š åˆ†å‰²**ä¼˜åŒ–å™¨çŠ¶æ€**ä¸**æ¢¯åº¦**ã€‚\n",
    "- **ZeRO-3**ï¼š åˆ†å‰²**ä¼˜åŒ–å™¨çŠ¶æ€**ã€**æ¢¯åº¦**ä¸**å‚æ•°**ã€‚\n",
    "\n",
    "![](./images/Code01ZeRO00.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a250f",
   "metadata": {},
   "source": [
    "\n",
    "å¯¹äºä½¿ç”¨ Adam ä¼˜åŒ–å™¨çš„æ¨¡å‹ï¼Œæ˜¾å­˜å ç”¨å¯ä¼°ç®—ä¸ºï¼š\n",
    "```\n",
    "æ€»æ˜¾å­˜ = å‚æ•°æ˜¾å­˜ + æ¢¯åº¦æ˜¾å­˜ + ä¼˜åŒ–å™¨çŠ¶æ€æ˜¾å­˜ + æ¿€æ´»å€¼æ˜¾å­˜\n",
    "å‚æ•°æ˜¾å­˜ = å‚æ•°é‡ Ã— 4 å­—èŠ‚ï¼ˆFP32ï¼‰\n",
    "æ¢¯åº¦æ˜¾å­˜ = å‚æ•°é‡ Ã— 4 å­—èŠ‚ï¼ˆFP32ï¼‰\n",
    "ä¼˜åŒ–å™¨çŠ¶æ€æ˜¾å­˜ = å‚æ•°é‡ Ã— 16 å­—èŠ‚ï¼ˆFP32 Adamï¼‰\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97271d95",
   "metadata": {},
   "source": [
    "æ˜¾å­˜å ç”¨åˆ†æå·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e400b934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆå§‹çŠ¶æ€: å·²åˆ†é…: 0.00GB, å˜åŒ–: +0.00GB\n",
      "æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.19GB, å˜åŒ–: +0.19GB\n",
      "ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.19GB, å˜åŒ–: +0.00GB\n",
      "æ•°æ®åŠ è½½å: å·²åˆ†é…: 0.19GB, å˜åŒ–: +0.00GB\n",
      "å‰å‘ä¼ æ’­å: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.01GB\n",
      "åå‘ä¼ æ’­å: å·²åˆ†é…: 0.39GB, å˜åŒ–: +0.19GB\n",
      "ä¼˜åŒ–å™¨æ›´æ–°å: å·²åˆ†é…: 0.77GB, å˜åŒ–: +0.38GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "class MemoryAnalyzer:\n",
    "    \"\"\"ç®€åŒ–çš„æ˜¾å­˜åˆ†æå·¥å…·ç±»ï¼ˆä»…ç”¨äºæ•™å­¦æ¼”ç¤ºï¼‰\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.memory_stats = defaultdict(list)\n",
    "        self.previous_allocated = 0\n",
    "\n",
    "    def record(self, tag=''):\n",
    "        \"\"\"è®°å½•å½“å‰ GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ˆå•ä½ï¼šGBï¼‰\"\"\"\n",
    "        torch.cuda.synchronize()  # ç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆ\n",
    "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024**3)\n",
    "        delta = allocated - self.previous_allocated\n",
    "        self.previous_allocated = allocated\n",
    "\n",
    "        self.memory_stats['allocated'].append(allocated)\n",
    "        self.memory_stats['reserved'].append(reserved)\n",
    "        self.memory_stats['delta'].append(delta)\n",
    "\n",
    "        print(f\"{tag}: å·²åˆ†é…: {allocated:.2f}GB, å˜åŒ–: {delta:+.2f}GB\")\n",
    "        return allocated\n",
    "\n",
    "# åˆ›å»ºæµ‹è¯•æ¨¡å‹\n",
    "def create_model(hidden_size=2048, num_layers=12):\n",
    "    \"\"\"åˆ›å»ºä¸€ä¸ªç®€å•çš„å…¨è¿æ¥æ¨¡å‹ç”¨äºæµ‹è¯•\"\"\"\n",
    "    layers = []\n",
    "    for _ in range(num_layers):\n",
    "        layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# æ‰§è¡Œæ˜¾å­˜åˆ†æ\n",
    "def analyze_memory(seed=42):\n",
    "    \"\"\"æ‰§è¡ŒåŸºç¡€è®­ç»ƒæµç¨‹å¹¶è®°å½•æ˜¾å­˜å˜åŒ–\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA ä¸å¯ç”¨\")\n",
    "        return None\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    analyzer = MemoryAnalyzer()\n",
    "\n",
    "    analyzer.record(\"åˆå§‹çŠ¶æ€\")\n",
    "\n",
    "    model = create_model().cuda()\n",
    "    analyzer.record(\"æ¨¡å‹åˆ›å»ºå\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    analyzer.record(\"ä¼˜åŒ–å™¨åˆ›å»ºå\")\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device='cuda')\n",
    "    targets = torch.randn(32, 2048, device='cuda')\n",
    "    analyzer.record(\"æ•°æ®åŠ è½½å\")\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = F.mse_loss(outputs, targets)\n",
    "    analyzer.record(\"å‰å‘ä¼ æ’­å\")\n",
    "\n",
    "    loss.backward()\n",
    "    analyzer.record(\"åå‘ä¼ æ’­å\")\n",
    "\n",
    "    optimizer.step()\n",
    "    analyzer.record(\"ä¼˜åŒ–å™¨æ›´æ–°å\")\n",
    "\n",
    "    return analyzer.memory_stats\n",
    "\n",
    "# æ‰§è¡Œåˆ†æ\n",
    "memory_stats = analyze_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d37206",
   "metadata": {},
   "source": [
    "é€šè¿‡è¿™ä¸ªåˆ†æå·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°åœ¨æ¯ä¸ªè®­ç»ƒé˜¶æ®µæ˜¾å­˜çš„ä½¿ç”¨æƒ…å†µå˜åŒ–ã€‚åœ¨å®é™…çš„å¤§æ¨¡å‹è®­ç»ƒä¸­ï¼Œè¿™äº›æ˜¾å­˜å ç”¨ä¼šæˆå€å¢é•¿ï¼Œå‡¸æ˜¾äº† ZeRO ä¼˜åŒ–çš„å¿…è¦æ€§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdae504",
   "metadata": {},
   "source": [
    "```\n",
    "åˆå§‹çŠ¶æ€: å·²åˆ†é…: 0.00GB, å˜åŒ–: +0.00GB\n",
    "æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.13GB\n",
    "ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.00GB\n",
    "æ•°æ®åŠ è½½å: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.00GB\n",
    "å‰å‘ä¼ æ’­å: å·²åˆ†é…: 0.14GB, å˜åŒ–: +0.01GB\n",
    "åå‘ä¼ æ’­å: å·²åˆ†é…: 0.27GB, å˜åŒ–: +0.13GB\n",
    "ä¼˜åŒ–å™¨æ›´æ–°å: å·²åˆ†é…: 0.52GB, å˜åŒ–: +0.25GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff08427",
   "metadata": {},
   "source": [
    "## 2. ZeRO-1: ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡\n",
    "\n",
    "ZeRO-1 é€šè¿‡å°†ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡åˆ°å¤šä¸ª GPU ä¸Šæ¥å‡å°‘æ˜¾å­˜å ç”¨ã€‚åœ¨ä¼ ç»Ÿæ•°æ®å¹¶è¡Œä¸­ï¼Œæ¯ä¸ª GPU éƒ½ä¿å­˜å®Œæ•´çš„ä¼˜åŒ–å™¨çŠ¶æ€å‰¯æœ¬ï¼Œè¿™é€ æˆäº†å¤§é‡çš„æ˜¾å­˜å†—ä½™ã€‚\n",
    "\n",
    "ZeRO-1 çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šæ¯ä¸ª GPU åªä¿å­˜ä¸€éƒ¨åˆ†ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå½“éœ€è¦æ›´æ–°å‚æ•°æ—¶ï¼Œé€šè¿‡é›†åˆé€šä¿¡æ“ä½œè·å–å®Œæ•´çš„æ¢¯åº¦ä¿¡æ¯ã€‚\n",
    "\n",
    "æ•°å­¦è¡¨è¾¾ä¸Šï¼Œå¯¹äº Adam ä¼˜åŒ–å™¨ï¼Œæ¯ä¸ª GPU åŸæœ¬éœ€è¦å­˜å‚¨ï¼š\n",
    "\n",
    "- å‚æ•°ï¼š$Î˜$\n",
    "- æ¢¯åº¦ï¼š$âˆ‡Î˜$\n",
    "- åŠ¨é‡ï¼š$m$\n",
    "- æ–¹å·®ï¼š$v$\n",
    "\n",
    "ZeRO-1 åˆ†ç‰‡åï¼Œæ¯ä¸ª GPU åªå­˜å‚¨ï¼š\n",
    "\n",
    "- å®Œæ•´å‚æ•°ï¼š$Î˜$\n",
    "- å®Œæ•´æ¢¯åº¦ï¼š$âˆ‡Î˜$\n",
    "- 1/N çš„åŠ¨é‡ï¼š$m_i$\n",
    "- 1/N çš„æ–¹å·®ï¼š$v_i$\n",
    "\n",
    "å…¶ä¸­ N æ˜¯ GPU æ•°é‡ã€‚\n",
    "\n",
    "![](./images/Code01ZeRO01.png)\n",
    "\n",
    "ZeRO-1 ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebac3fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.20GB\n",
      "ZeRO-1 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB\n",
      "è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.77GB, å˜åŒ–: +0.56GB\n"
     ]
    }
   ],
   "source": [
    "class Zero1Optimizer:\n",
    "    \"\"\"ZeRO-1 æ¨¡æ‹Ÿå®ç°ï¼šå°†å‚æ•°åˆ†ç‰‡ï¼Œæ¯ä¸ªåˆ†ç‰‡ç‹¬ç«‹ä¼˜åŒ–å™¨ï¼ˆå• GPU æ¨¡æ‹Ÿï¼‰\"\"\"\n",
    "\n",
    "    def __init__(self, params, optimizer_class=torch.optim.Adam, num_shards=4, **kwargs):\n",
    "        self.params = list(params)\n",
    "        self.num_shards = num_shards\n",
    "\n",
    "        # å°†å‚æ•°å‡åŒ€åˆ†ç‰‡\n",
    "        self.shards = []\n",
    "        shard_size = (len(self.params) + num_shards - 1) // num_shards\n",
    "        for i in range(0, len(self.params), shard_size):\n",
    "            self.shards.append(self.params[i:i + shard_size])\n",
    "\n",
    "        # ä¸ºæ¯ä¸ªåˆ†ç‰‡åˆ›å»ºç‹¬ç«‹ä¼˜åŒ–å™¨\n",
    "        self.optimizers = [\n",
    "            optimizer_class(shard, **kwargs) for shard in self.shards\n",
    "        ]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        # æ¯ä¸ªåˆ†ç‰‡ç‹¬ç«‹æ›´æ–°ï¼ˆæ¨¡æ‹Ÿå¤š GPU å„è‡ªæ›´æ–°è‡ªå·±çš„åˆ†ç‰‡ï¼‰\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()\n",
    "\n",
    "# æµ‹è¯• ZeRO-1 æ•ˆæœ\n",
    "def test_zero1(seed=42):\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    analyzer = MemoryAnalyzer()\n",
    "\n",
    "    model = create_model().cuda()\n",
    "    analyzer.record(\"æ¨¡å‹åˆ›å»ºå\")\n",
    "\n",
    "    optimizer = Zero1Optimizer(model.parameters(), num_shards=4, lr=1e-3)\n",
    "    analyzer.record(\"ZeRO-1 ä¼˜åŒ–å™¨åˆ›å»ºå\")\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device='cuda')\n",
    "    outputs = model(inputs)\n",
    "    loss = F.mse_loss(outputs, torch.randn_like(outputs))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    analyzer.record(\"è®­ç»ƒä¸€æ­¥å\")\n",
    "\n",
    "    return analyzer.memory_stats\n",
    "\n",
    "# æ‰§è¡Œæµ‹è¯•\n",
    "zero1_stats = test_zero1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea50a4",
   "metadata": {},
   "source": [
    "è¿™ä¸ªç®€åŒ–å®ç°å±•ç¤ºäº† ZeRO-1 çš„æ ¸å¿ƒæ€æƒ³ï¼šæ¯ä¸ª GPU åªå­˜å‚¨å’Œæ›´æ–°ä¸€éƒ¨åˆ†å‚æ•°çš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œé€šè¿‡é€šä¿¡æ“ä½œç¡®ä¿æ‰€æœ‰ GPU çš„å‚æ•°ä¿æŒä¸€è‡´ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fabfd7",
   "metadata": {},
   "source": [
    "```\n",
    "æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.14GB, å˜åŒ–: +0.14GB\n",
    "ZeRO-1 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.14GB, å˜åŒ–: +0.00GB\n",
    "è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.52GB, å˜åŒ–: +0.38GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8870f",
   "metadata": {},
   "source": [
    "## 3. ZeRO-2: æ¢¯åº¦åˆ†ç‰‡\n",
    "\n",
    "ZeRO-2 åœ¨ ZeRO-1 çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œä¸ä»…åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œè¿˜åˆ†ç‰‡æ¢¯åº¦ã€‚è¿™è¿›ä¸€æ­¥å‡å°‘äº†æ˜¾å­˜å ç”¨ï¼Œå› ä¸ºæ¢¯åº¦é€šå¸¸ä¸å‚æ•°å¤§å°ç›¸åŒã€‚\n",
    "\n",
    "![](./images/Code01ZeRO02.png)\n",
    "\n",
    "åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ª GPU è®¡ç®—å…¶åˆ†é…åˆ°çš„å‚æ•°çš„æ¢¯åº¦ï¼Œç„¶åé€šè¿‡ Reduce-Scatter æ“ä½œèšåˆæ¢¯åº¦ã€‚è¿™æ ·æ¯ä¸ª GPU åªä¿å­˜ä¸€éƒ¨åˆ†æ¢¯åº¦ï¼Œè€Œä¸æ˜¯å…¨éƒ¨æ¢¯åº¦ã€‚æ¢¯åº¦åˆ†ç‰‡çš„æ•°å­¦è¡¨è¾¾ï¼š\n",
    "\n",
    "- ä¼ ç»Ÿæ–¹æ³•ï¼šæ¯ä¸ª GPU å­˜å‚¨å®Œæ•´æ¢¯åº¦ $âˆ‡Î˜$\n",
    "- ZeRO-2ï¼šæ¯ä¸ª GPU å­˜å‚¨ 1/N çš„æ¢¯åº¦ $âˆ‡Î˜_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410532f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.20GB\n",
      "ZeRO-2 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB\n",
      "è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.34GB, å˜åŒ–: +0.14GB\n"
     ]
    }
   ],
   "source": [
    "class Zero2Optimizer(Zero1Optimizer):\n",
    "    \"\"\"ZeRO-2 æ¨¡æ‹Ÿï¼šåœ¨ ZeRO-1 åŸºç¡€ä¸Šï¼Œåªä¿ç•™å½“å‰åˆ†ç‰‡çš„æ¢¯åº¦\"\"\"\n",
    "\n",
    "    def step(self):\n",
    "        current_shard_idx = 0  # å‡è®¾å½“å‰ GPU è´Ÿè´£ç¬¬ 0 ä¸ªåˆ†ç‰‡\n",
    "\n",
    "        # åˆ é™¤éæœ¬åˆ†ç‰‡çš„æ¢¯åº¦ï¼ˆæ¨¡æ‹Ÿ reduce-scatter åé‡Šæ”¾ï¼‰\n",
    "        for shard_idx, shard in enumerate(self.shards):\n",
    "            for param in shard:\n",
    "                if param.grad is not None:\n",
    "                    if shard_idx != current_shard_idx:\n",
    "                        param.grad = None  # âœ… é‡Šæ”¾æ¢¯åº¦æ˜¾å­˜\n",
    "\n",
    "        # ä»…æ›´æ–°æœ¬åˆ†ç‰‡\n",
    "        self.optimizers[current_shard_idx].step()\n",
    "\n",
    "# æµ‹è¯• ZeRO-2 æ•ˆæœ\n",
    "def test_zero2(seed=42):\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    analyzer = MemoryAnalyzer()\n",
    "\n",
    "    model = create_model().cuda()\n",
    "    analyzer.record(\"æ¨¡å‹åˆ›å»ºå\")\n",
    "\n",
    "    optimizer = Zero2Optimizer(model.parameters(), num_shards=4, lr=1e-3)\n",
    "    analyzer.record(\"ZeRO-2 ä¼˜åŒ–å™¨åˆ›å»ºå\")\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device='cuda')\n",
    "    outputs = model(inputs)\n",
    "    loss = F.mse_loss(outputs, torch.randn_like(outputs))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    analyzer.record(\"è®­ç»ƒä¸€æ­¥å\")\n",
    "\n",
    "    return analyzer.memory_stats\n",
    "\n",
    "zero2_stats = test_zero2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6a0f9",
   "metadata": {},
   "source": [
    "ZeRO-2 é€šè¿‡æ¢¯åº¦åˆ†ç‰‡è¿›ä¸€æ­¥å‡å°‘äº†æ˜¾å­˜å ç”¨ï¼Œä½†å¢åŠ äº†é€šä¿¡å¼€é”€ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦æ ¹æ®ç½‘ç»œå¸¦å®½å’Œè®¡ç®—èƒ½åŠ›æƒè¡¡è¿™ç§æƒè¡¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08975588",
   "metadata": {},
   "source": [
    "```\n",
    "æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.13GB\n",
    "ZeRO-2 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.00GB\n",
    "è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.31GB, å˜åŒ–: +0.18GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40d456",
   "metadata": {},
   "source": [
    "## 4. ZeRO-3: å‚æ•°åˆ†ç‰‡\n",
    "\n",
    "ZeRO-3 æ˜¯ ZeRO ç³»åˆ—çš„æœ€ç»ˆå½¢æ€ï¼Œå®ƒä¸ä»…åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼Œè¿˜åˆ†ç‰‡æ¨¡å‹å‚æ•°æœ¬èº«ã€‚è¿™æ„å‘³ç€æ¯ä¸ª GPU åªå­˜å‚¨æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œå¤§å¤§é™ä½äº†å•ä¸ª GPU çš„æ˜¾å­˜éœ€æ±‚ã€‚\n",
    "\n",
    "![](./images/Code01ZeRO03.png)\n",
    "\n",
    "ZeRO-3 çš„å·¥ä½œåŸç†ï¼š\n",
    "\n",
    "1. å‰å‘ä¼ æ’­æ—¶ï¼Œæ¯ä¸ª GPU åªè®¡ç®—å®ƒæ‹¥æœ‰çš„å‚æ•°éƒ¨åˆ†\n",
    "2. éœ€è¦å…¶ä»– GPU çš„å‚æ•°æ—¶ï¼Œé€šè¿‡é€šä¿¡æ“ä½œè·å–\n",
    "3. åå‘ä¼ æ’­æ—¶ç±»ä¼¼ï¼Œåªè®¡ç®—æœ¬åœ°å‚æ•°çš„æ¢¯åº¦\n",
    "4. é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„é€šä¿¡æ¨¡å¼æœ€å°åŒ–é€šä¿¡å¼€é”€\n",
    "\n",
    "å‚æ•°åˆ†ç‰‡çš„æ•°å­¦è¡¨è¾¾ï¼š\n",
    "\n",
    "- ä¼ ç»Ÿæ–¹æ³•ï¼šæ¯ä¸ª GPU å­˜å‚¨å®Œæ•´å‚æ•° $Î˜$\n",
    "- ZeRO-3ï¼šæ¯ä¸ª GPU å­˜å‚¨ 1/N çš„å‚æ•° $Î˜_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa67c7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeRO-3 æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.06GB, å˜åŒ–: +0.06GB\n",
      "ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.06GB, å˜åŒ–: +0.00GB\n",
      "è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.14GB\n"
     ]
    }
   ],
   "source": [
    "class Zero3Model(nn.Module):\n",
    "    \"\"\"ZeRO-3 æ¨¡æ‹Ÿï¼šä»…åŠ è½½æ¨¡å‹çš„ä¸€éƒ¨åˆ†å±‚ï¼ˆå‚æ•°åˆ†ç‰‡ï¼‰\"\"\"\n",
    "\n",
    "    def __init__(self, base_model, shard_id=0, num_shards=4):\n",
    "        super().__init__()\n",
    "        self.shard_id = shard_id\n",
    "        self.num_shards = num_shards\n",
    "\n",
    "        # è®¡ç®—å½“å‰åˆ†ç‰‡è´Ÿè´£çš„å±‚èŒƒå›´\n",
    "        total_layers = len(base_model)\n",
    "        layers_per_shard = (total_layers + num_shards - 1) // num_shards\n",
    "        start = shard_id * layers_per_shard\n",
    "        end = min(start + layers_per_shard, total_layers)\n",
    "\n",
    "        # ä»…ä¿ç•™æœ¬åˆ†ç‰‡çš„å±‚\n",
    "        self.layers = nn.ModuleList([base_model[i] for i in range(start, end)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# æµ‹è¯• ZeRO-3 æ•ˆæœ\n",
    "def test_zero3(seed=42):\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    analyzer = MemoryAnalyzer()\n",
    "\n",
    "    base_model = create_model()\n",
    "    model = Zero3Model(base_model, shard_id=0, num_shards=4).cuda()\n",
    "    analyzer.record(\"ZeRO-3 æ¨¡å‹åˆ›å»ºå\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    analyzer.record(\"ä¼˜åŒ–å™¨åˆ›å»ºå\")\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device='cuda')\n",
    "    outputs = model(inputs)\n",
    "    loss = F.mse_loss(outputs, torch.randn_like(outputs))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    analyzer.record(\"è®­ç»ƒä¸€æ­¥å\")\n",
    "\n",
    "    return analyzer.memory_stats\n",
    "\n",
    "# æ‰§è¡Œæµ‹è¯•\n",
    "zero3_stats = test_zero3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc174396",
   "metadata": {},
   "source": [
    "ZeRO-3 æä¾›äº†æœ€å¤§çš„æ˜¾å­˜èŠ‚çœï¼Œä½†é€šä¿¡å¼€é”€ä¹Ÿæœ€å¤§ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€šå¸¸éœ€è¦ç»“åˆå„ç§ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚é€šä¿¡è®¡ç®—é‡å ã€æ¢¯åº¦ç´¯ç§¯ç­‰ï¼Œæ¥å¹³è¡¡æ˜¾å­˜èŠ‚çœå’Œè®­ç»ƒé€Ÿåº¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb53782",
   "metadata": {},
   "source": [
    "```\n",
    "ZeRO-3 æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.03GB, å˜åŒ–: +0.03GB\n",
    "ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.03GB, å˜åŒ–: +0.00GB\n",
    "è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.11GB, å˜åŒ–: +0.08GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1b4978",
   "metadata": {},
   "source": [
    "## 5. Zero Offload æŠ€æœ¯\n",
    "\n",
    "Zero Offload æŠ€æœ¯å°†ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°å¸è½½åˆ° CPU å†…å­˜æˆ– NVMe å­˜å‚¨ï¼Œè¿›ä¸€æ­¥æ‰©å±•äº†å¯è®­ç»ƒçš„æ¨¡å‹è§„æ¨¡ã€‚è¿™ç§æŠ€æœ¯ç‰¹åˆ«é€‚åˆåœ¨æœ‰é™ GPU å†…å­˜ç¯å¢ƒä¸‹è®­ç»ƒè¶…å¤§æ¨¡å‹ã€‚\n",
    "\n",
    "![](./images/Code01ZeRO04.png)\n",
    "\n",
    "Offload çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ CPU å†…å­˜å’Œ NVMe å­˜å‚¨ä½œä¸º GPU æ˜¾å­˜çš„æ‰©å±•ï¼Œé€šè¿‡å¼‚æ­¥æ•°æ®ä¼ è¾“å’Œè®¡ç®—é‡å æ¥æœ€å°åŒ–æ€§èƒ½å½±å“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c355a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.20GB\n",
      "CPU Offload ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB\n",
      "è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.39GB, å˜åŒ–: +0.19GB\n"
     ]
    }
   ],
   "source": [
    "class CPUOffloadOptimizer:\n",
    "    \"\"\"CPU Offload æ¨¡æ‹Ÿï¼šä¼˜åŒ–å™¨çŠ¶æ€å­˜å‚¨åœ¨ CPU\"\"\"\n",
    "\n",
    "    def __init__(self, params, optimizer_class=torch.optim.Adam, **kwargs):\n",
    "        self.gpu_params = list(params)\n",
    "        # åœ¨ CPU ä¸Šåˆ›å»ºå‚æ•°å‰¯æœ¬ï¼ˆæ— æ¢¯åº¦ï¼‰\n",
    "        self.cpu_params = [p.detach().cpu().clone() for p in self.gpu_params]\n",
    "        self.optimizer = optimizer_class(self.cpu_params, **kwargs)\n",
    "\n",
    "    def step(self):\n",
    "        # æ¢¯åº¦ä» GPU â†’ CPU\n",
    "        for gpu_p, cpu_p in zip(self.gpu_params, self.cpu_params):\n",
    "            if gpu_p.grad is not None:\n",
    "                cpu_p.grad = gpu_p.grad.cpu()\n",
    "\n",
    "        # åœ¨ CPU ä¸Šæ›´æ–°\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # å‚æ•°ä» CPU â†’ GPU\n",
    "        for gpu_p, cpu_p in zip(self.gpu_params, self.cpu_params):\n",
    "            gpu_p.data.copy_(cpu_p.data)\n",
    "\n",
    "        # æ¸…ç† CPU æ¢¯åº¦\n",
    "        for cpu_p in self.cpu_params:\n",
    "            cpu_p.grad = None\n",
    "\n",
    "# æµ‹è¯• CPU Offload æ•ˆæœ\n",
    "def test_cpu_offload(seed=42):\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.empty_cache()\n",
    "    analyzer = MemoryAnalyzer()\n",
    "\n",
    "    model = create_model().cuda()\n",
    "    analyzer.record(\"æ¨¡å‹åˆ›å»ºå\")\n",
    "\n",
    "    optimizer = CPUOffloadOptimizer(model.parameters(), lr=1e-3)\n",
    "    analyzer.record(\"CPU Offload ä¼˜åŒ–å™¨åˆ›å»ºå\")\n",
    "\n",
    "    inputs = torch.randn(32, 2048, device='cuda')\n",
    "    outputs = model(inputs)\n",
    "    loss = F.mse_loss(outputs, torch.randn_like(outputs))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    analyzer.record(\"è®­ç»ƒä¸€æ­¥å\")\n",
    "\n",
    "    return analyzer.memory_stats\n",
    "\n",
    "# æ‰§è¡Œæµ‹è¯•\n",
    "offload_stats = test_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae4922",
   "metadata": {},
   "source": [
    "```\n",
    "æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.13GB\n",
    "CPU Offload ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.00GB\n",
    "è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.25GB, å˜åŒ–: +0.12GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacbd31",
   "metadata": {},
   "source": [
    "## 6. æ€§èƒ½åˆ†æä¸å®éªŒç»“æœ\n",
    "\n",
    "ä¸ºäº†éªŒè¯ ZeRO å„çº§åˆ«çš„æ•ˆæœï¼Œæˆ‘ä»¬è®¾è®¡äº†ä»¥ä¸‹å®éªŒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe058aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " æ˜¾å­˜ä½¿ç”¨å¯¹æ¯” (å•ä½: GB):\n",
      "----------------------------------------\n",
      "åˆå§‹çŠ¶æ€: å·²åˆ†é…: 0.02GB, å˜åŒ–: +0.02GB\n",
      "æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.19GB\n",
      "ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB\n",
      "æ•°æ®åŠ è½½å: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB\n",
      "å‰å‘ä¼ æ’­å: å·²åˆ†é…: 0.21GB, å˜åŒ–: +0.00GB\n",
      "åå‘ä¼ æ’­å: å·²åˆ†é…: 0.39GB, å˜åŒ–: +0.18GB\n",
      "ä¼˜åŒ–å™¨æ›´æ–°å: å·²åˆ†é…: 0.77GB, å˜åŒ–: +0.38GB\n",
      "----------------------------------------\n",
      "æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.20GB\n",
      "ZeRO-1 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB\n",
      "è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.77GB, å˜åŒ–: +0.56GB\n",
      "----------------------------------------\n",
      "æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.20GB\n",
      "ZeRO-2 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB\n",
      "è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.34GB, å˜åŒ–: +0.14GB\n",
      "----------------------------------------\n",
      "ZeRO-3 æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.06GB, å˜åŒ–: +0.06GB\n",
      "ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.06GB, å˜åŒ–: +0.00GB\n",
      "è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.14GB\n",
      "----------------------------------------\n",
      "æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.20GB\n",
      "CPU Offload ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB\n",
      "è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.39GB, å˜åŒ–: +0.19GB\n",
      "----------------------------------------\n",
      "åŸºç¡€æ–¹æ³•: 0.77GB\n",
      "ZeRO-1: 0.77GB (0.0% èŠ‚çœ)\n",
      "ZeRO-2: 0.34GB (55.0% èŠ‚çœ)\n",
      "ZeRO-3: 0.20GB (73.4% èŠ‚çœ)\n",
      "CPU Offload: 0.39GB (48.9% èŠ‚çœ)\n"
     ]
    }
   ],
   "source": [
    "# æ±‡æ€»æ‰€æœ‰æ–¹æ³•çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ\n",
    "def compare_methods():\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "\n",
    "    print(\"\\n æ˜¾å­˜ä½¿ç”¨å¯¹æ¯” (å•ä½: GB):\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # é‡æ–°è¿è¡ŒåŸºç¡€æµ‹è¯•\n",
    "    baseline = analyze_memory()\n",
    "    print(\"-\" * 40)\n",
    "    zero1 = test_zero1()\n",
    "    print(\"-\" * 40)\n",
    "    zero2 = test_zero2()\n",
    "    print(\"-\" * 40)\n",
    "    zero3 = test_zero3()\n",
    "    print(\"-\" * 40)\n",
    "    offload = test_cpu_offload()\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # æå–æœ€ç»ˆæ˜¾å­˜ä½¿ç”¨é‡\n",
    "    print(f\"åŸºç¡€æ–¹æ³•: {baseline['allocated'][-1]:.2f}GB\")\n",
    "    print(f\"ZeRO-1: {zero1['allocated'][-1]:.2f}GB ({(1-zero1['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% èŠ‚çœ)\")\n",
    "    print(f\"ZeRO-2: {zero2['allocated'][-1]:.2f}GB ({(1-zero2['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% èŠ‚çœ)\")\n",
    "    print(f\"ZeRO-3: {zero3['allocated'][-1]:.2f}GB ({(1-zero3['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% èŠ‚çœ)\")\n",
    "    print(f\"CPU Offload: {offload['allocated'][-1]:.2f}GB ({(1-offload['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% èŠ‚çœ)\")\n",
    "\n",
    "# æ‰§è¡Œå¯¹æ¯”\n",
    "compare_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a33059",
   "metadata": {},
   "source": [
    "é€šè¿‡è¿™ä¸ªå®éªŒï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ° ZeRO å„çº§åˆ«å¯¹æ˜¾å­˜å ç”¨çš„ä¼˜åŒ–æ•ˆæœã€‚åœ¨å®é™…çš„å¤§æ¨¡å‹è®­ç»ƒä¸­ï¼Œè¿™äº›ä¼˜åŒ–å¯ä»¥å¸¦æ¥æ•°å€ç”šè‡³æ•°åå€çš„æ˜¾å­˜èŠ‚çœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcbd6f1",
   "metadata": {},
   "source": [
    "```\n",
    "æ˜¾å­˜ä½¿ç”¨å¯¹æ¯” (å•ä½: GB):\n",
    "----------------------------------------\n",
    "åŸºç¡€æ–¹æ³•: 0.39GB\n",
    "ZeRO-1: 0.39GB (0.0% èŠ‚çœ)\n",
    "ZeRO-2: 0.31GB (20.5% èŠ‚çœ)\n",
    "ZeRO-3: 0.11GB (71.8% èŠ‚çœ)\n",
    "CPU Offload: 0.25GB (35.9% èŠ‚çœ)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a22f1",
   "metadata": {},
   "source": [
    "## æ€»ç»“ä¸æ€è€ƒ\n",
    "\n",
    "ZeRO æŠ€æœ¯é€šè¿‡åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ï¼Œæ˜¾è‘—é™ä½äº†å¤§æ¨¡å‹è®­ç»ƒçš„æ˜¾å­˜éœ€æ±‚ã€‚æœ¬å®éªŒé€šè¿‡ä»£ç å®ç°å’ŒåŸç†åˆ†æï¼Œæ·±å…¥æ¢è®¨äº†ï¼š\n",
    "\n",
    "1. **ZeRO-1**ï¼šä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ï¼Œå‡å°‘çº¦ 4 å€æ˜¾å­˜å ç”¨\n",
    "2. **ZeRO-2**ï¼šæ¢¯åº¦åˆ†ç‰‡ï¼Œè¿›ä¸€æ­¥å‡å°‘çº¦ 8 å€æ˜¾å­˜å ç”¨  \n",
    "3. **ZeRO-3**ï¼šå‚æ•°åˆ†ç‰‡ï¼Œæœ€å¤§å¯å‡å°‘çº¦ N å€æ˜¾å­˜å ç”¨ï¼ˆN ä¸º GPU æ•°é‡ï¼‰\n",
    "4. **Zero Offload**ï¼šå°†æ•°æ®å¸è½½åˆ° CPU/NVMeï¼Œæ”¯æŒè®­ç»ƒè¶…å¤§æ¨¡å‹\n",
    "\n",
    "è¿™äº›æŠ€æœ¯å¯ä»¥ç»„åˆä½¿ç”¨ï¼Œæ ¹æ®å…·ä½“çš„ç¡¬ä»¶ç¯å¢ƒå’Œæ¨¡å‹å¤§å°é€‰æ‹©æœ€åˆé€‚çš„é…ç½®ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒDeepSpeed æ¡†æ¶æä¾›äº†å®Œæ•´çš„ ZeRO å®ç°ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨ç»è¿‡ä¼˜åŒ–çš„å®˜æ–¹å®ç°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a71e6",
   "metadata": {},
   "source": [
    "## å¼•ç”¨ä¸å‚è€ƒ\n",
    "\n",
    "- https://arxiv.org/abs/1910.02054\n",
    "- https://www.cnblogs.com/whiteBear/p/18341975"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
