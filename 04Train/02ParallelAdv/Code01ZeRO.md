<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->

# CODE 01: ZeRO æ˜¾å­˜ä¼˜åŒ–å®è·µ

> Author by: è®¸ç¿å²·

ç›®å‰**GPU + Pytorch + Megatron + DeepSpeed**æ˜¯å¸¸ç”¨çš„è®­ç»ƒè¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ¡†æ¶ã€‚è€Œå¾®è½¯å¼€å‘çš„**DeepSpeed**çš„æ ¸å¿ƒå°±æ˜¯**ZeRO**(Zero Redundancy Optimizer)ï¼Œå®ƒæ˜¯ä¸€ç§æ˜¾å­˜ä¼˜åŒ–çš„**æ•°æ®å¹¶è¡Œ**(data parallelismï¼ŒDP)æ–¹æ¡ˆã€‚**ZeRO**æŠ€æœ¯é€šè¿‡æ¶ˆé™¤**æ•°æ®å¹¶è¡Œ**ä¸­çš„æ˜¾å­˜å†—ä½™ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒå¤§æ¨¡å‹æ‰€éœ€çš„æ˜¾å­˜ã€‚

æœ¬å®éªŒå°†æ·±å…¥æ¢è®¨ ZeRO çš„å„çº§ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡å®é™…ä»£ç æ¼”ç¤ºå’Œåˆ†æï¼Œç†è§£ä¸åŒçº§åˆ«çš„ ZeRO å¦‚ä½•å®ç°æ˜¾å­˜ä¼˜åŒ–ã€‚

ğŸ“Œ **PS**ï¼šæœ¬ Notebook **ä»…ç”¨äºæ•™å­¦ç›®çš„**ï¼Œæ‰€æœ‰ ZeRO å®ç°å‡ä¸º**å• GPU ä¸Šçš„ç®€åŒ–æ¨¡æ‹Ÿ**ï¼Œ**å¹¶æœªä½¿ç”¨çœŸå®å¤š GPU å¹¶è¡Œæˆ–é€šä¿¡åŸè¯­**ï¼ˆå¦‚ `all_reduce`, `reduce_scatter`ï¼‰ã€‚çœŸå® ZeRO éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼ˆå¦‚ DeepSpeed + å¤š GPUï¼‰ï¼Œå…¶æ˜¾å­˜èŠ‚çœæ•ˆæœåœ¨ **N ä¸ª GPU æ—¶æ‰ä½“ç°ä¸º 1/N**ã€‚æœ¬å®éªŒé€šè¿‡â€œäººä¸ºåˆ†ç‰‡ + æ‰‹åŠ¨é‡Šæ”¾â€æ¥**æ¨¡æ‹Ÿ**åˆ†ç‰‡è¡Œä¸ºï¼Œå¸®åŠ©ç†è§£æ ¸å¿ƒæ€æƒ³ã€‚



## 1. æ¨¡å‹æ˜¾å­˜å ç”¨åˆ†æ

åœ¨æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­ï¼Œæ˜¾å­˜å ç”¨å¯ä»¥åˆ†ä¸º**Activation**å’Œ**Model State**ä¸¤éƒ¨åˆ†ï¼š

**Activation**ï¼š
- **ä¸­é—´æ¿€æ´»å€¼**ï¼ˆActivationsï¼‰ï¼šåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚ä¼šäº§ç”Ÿä¸­é—´æ¿€æ´»å€¼ï¼Œè¿™äº›æ¿€æ´»å€¼éœ€è¦å†åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ç”¨æ¥è®¡ç®—æ¢¯åº¦ã€‚
- **è¾“å…¥æ•°æ®**ï¼ˆInputs Dataï¼‰ï¼šæ‰¹å¤„ç†ä¸­è¾“å…¥æ•°æ®ä¹Ÿå ç”¨æ˜¾å­˜ï¼Œå°¤å…¶æ˜¯å½“æ‰¹å¤„ç†è¾ƒå¤§æ—¶ã€‚

**Model State**ï¼š

- **ä¼˜åŒ–å™¨çŠ¶æ€**ï¼ˆOptimizer Statesï¼‰ï¼šæ˜¯ Optimizer åœ¨è¿›è¡Œæ¢¯åº¦æ›´æ–°æ—¶æ‰€éœ€è¦ç”¨åˆ°æ•°æ®ã€‚ä¸€äº›ä¼˜åŒ–å™¨(å¦‚ Adam)éœ€è¦å­˜å‚¨é¢å¤–çš„çŠ¶æ€ä¿¡æ¯ï¼Œå¦‚æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡å€¼å’Œå¹³æ–¹æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡å€¼ã€‚ä¾‹å¦‚ SGD ä¸­çš„ Momentum,å³ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒæ—¶çš„**Float32 Master Parameters**ã€‚
- **æ¨¡å‹å‚æ•°**ï¼ˆParametersï¼‰ï¼šæ¨¡å‹çš„å¯å­¦ä¹ æƒé‡ï¼Œå¦‚å­˜å‚¨åœ¨æ˜¾å­˜ä¸­çš„æ¨¡å‹æƒé‡å’Œåç½®é¡¹ã€‚
- **æ¢¯åº¦**ï¼ˆGradientsï¼‰ï¼šåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦ï¼Œç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°ã€‚å…¶å†³å®šäº†å‚æ•°çš„æ›´æ–°æ–¹å‘ã€‚

å®ƒä»¬ä¸‰ä¸ªç®€ç§°**OPG**ï¼Œå…¶ä¸­**ä¼˜åŒ–å™¨çŠ¶æ€**ä¼šå æ®å¤§çº¦ 2 å€å‚æ•°é‡çš„æ˜¾å­˜ç©ºé—´ï¼Œè¿™å–å†³äºé€‰æ‹©çš„ä¼˜åŒ–å™¨ï¼Œä¹Ÿæ˜¯æ•´ä¸ªè®­ç»ƒä¸­å æ®æœ€å¤§ç©ºé—´çš„éƒ¨åˆ†ã€‚

**ZeRO**åˆ™åœ¨æ•°æ®å¹¶è¡Œçš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†å¯¹å†—ä½™**Model States**çš„ä¼˜åŒ–ã€‚ä½¿ç”¨ ZeRO åï¼Œå„ä¸ªè¿›ç¨‹ä¹‹ååªä¿å­˜å®Œæ•´çŠ¶æ€çš„**1/GPUs**ï¼Œäº’ä¸é‡å ï¼Œä¸å†å­˜åœ¨å†—ä½™ã€‚ç›¸æ¯”ä¼ ç»Ÿæ•°æ®å¹¶è¡Œçš„ç®€å•å¤åˆ¶ï¼Œ**ZeRO**é€šè¿‡å°†æ¨¡å‹çš„**å‚æ•°**ã€**æ¢¯åº¦** å’Œ **ä¼˜åŒ–å™¨çŠ¶æ€**åˆ’åˆ†åˆ°ä¸åŒçš„è¿›ç¨‹æ¥æ¶ˆé™¤å†—ä½™çš„å†…å­˜å ç”¨ï¼Œä¹Ÿå°±å¼•å‡º**ZeRO**çš„ä¸‰ä¸ªä¸åŒçš„çº§åˆ«,åˆ†åˆ«å¯¹åº”**Model States**ä¸åŒç¨‹åº¦çš„åˆ†å‰²(Partition)ï¼š

- **ZeRO-1**ï¼š åˆ†å‰²**ä¼˜åŒ–å™¨çŠ¶æ€**ã€‚
- **ZeRO-2**ï¼š åˆ†å‰²**ä¼˜åŒ–å™¨çŠ¶æ€**ä¸**æ¢¯åº¦**ã€‚
- **ZeRO-3**ï¼š åˆ†å‰²**ä¼˜åŒ–å™¨çŠ¶æ€**ã€**æ¢¯åº¦**ä¸**å‚æ•°**ã€‚

![](./images/Code01ZeRO00.png)


å¯¹äºä½¿ç”¨ Adam ä¼˜åŒ–å™¨çš„æ¨¡å‹ï¼Œæ˜¾å­˜å ç”¨å¯ä¼°ç®—ä¸ºï¼š
```
æ€»æ˜¾å­˜ = å‚æ•°æ˜¾å­˜ + æ¢¯åº¦æ˜¾å­˜ + ä¼˜åŒ–å™¨çŠ¶æ€æ˜¾å­˜ + æ¿€æ´»å€¼æ˜¾å­˜
å‚æ•°æ˜¾å­˜ = å‚æ•°é‡ Ã— 4 å­—èŠ‚ï¼ˆFP32ï¼‰
æ¢¯åº¦æ˜¾å­˜ = å‚æ•°é‡ Ã— 4 å­—èŠ‚ï¼ˆFP32ï¼‰
ä¼˜åŒ–å™¨çŠ¶æ€æ˜¾å­˜ = å‚æ•°é‡ Ã— 16 å­—èŠ‚ï¼ˆFP32 Adamï¼‰
```

æ˜¾å­˜å ç”¨åˆ†æå·¥å…·


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import defaultdict

class MemoryAnalyzer:
    """ç®€åŒ–çš„æ˜¾å­˜åˆ†æå·¥å…·ç±»ï¼ˆä»…ç”¨äºæ•™å­¦æ¼”ç¤ºï¼‰"""

    def __init__(self):
        self.memory_stats = defaultdict(list)
        self.previous_allocated = 0

    def record(self, tag=''):
        """è®°å½•å½“å‰ GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ˆå•ä½ï¼šGBï¼‰"""
        torch.cuda.synchronize()  # ç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆ
        allocated = torch.cuda.memory_allocated() / (1024**3)
        reserved = torch.cuda.memory_reserved() / (1024**3)
        delta = allocated - self.previous_allocated
        self.previous_allocated = allocated

        self.memory_stats['allocated'].append(allocated)
        self.memory_stats['reserved'].append(reserved)
        self.memory_stats['delta'].append(delta)

        print(f"{tag}: å·²åˆ†é…: {allocated:.2f}GB, å˜åŒ–: {delta:+.2f}GB")
        return allocated

# åˆ›å»ºæµ‹è¯•æ¨¡å‹
def create_model(hidden_size=2048, num_layers=12):
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„å…¨è¿æ¥æ¨¡å‹ç”¨äºæµ‹è¯•"""
    layers = []
    for _ in range(num_layers):
        layers.append(nn.Linear(hidden_size, hidden_size))
        layers.append(nn.ReLU())
    return nn.Sequential(*layers)

# æ‰§è¡Œæ˜¾å­˜åˆ†æ
def analyze_memory(seed=42):
    """æ‰§è¡ŒåŸºç¡€è®­ç»ƒæµç¨‹å¹¶è®°å½•æ˜¾å­˜å˜åŒ–"""
    if not torch.cuda.is_available():
        print("CUDA ä¸å¯ç”¨")
        return None

    torch.manual_seed(seed)
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    analyzer = MemoryAnalyzer()

    analyzer.record("åˆå§‹çŠ¶æ€")

    model = create_model().cuda()
    analyzer.record("æ¨¡å‹åˆ›å»ºå")

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    analyzer.record("ä¼˜åŒ–å™¨åˆ›å»ºå")

    inputs = torch.randn(32, 2048, device='cuda')
    targets = torch.randn(32, 2048, device='cuda')
    analyzer.record("æ•°æ®åŠ è½½å")

    outputs = model(inputs)
    loss = F.mse_loss(outputs, targets)
    analyzer.record("å‰å‘ä¼ æ’­å")

    loss.backward()
    analyzer.record("åå‘ä¼ æ’­å")

    optimizer.step()
    analyzer.record("ä¼˜åŒ–å™¨æ›´æ–°å")

    return analyzer.memory_stats

# æ‰§è¡Œåˆ†æ
memory_stats = analyze_memory()
```

    åˆå§‹çŠ¶æ€: å·²åˆ†é…: 0.00GB, å˜åŒ–: +0.00GB
    æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.19GB, å˜åŒ–: +0.19GB
    ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.19GB, å˜åŒ–: +0.00GB
    æ•°æ®åŠ è½½å: å·²åˆ†é…: 0.19GB, å˜åŒ–: +0.00GB
    å‰å‘ä¼ æ’­å: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.01GB
    åå‘ä¼ æ’­å: å·²åˆ†é…: 0.39GB, å˜åŒ–: +0.19GB
    ä¼˜åŒ–å™¨æ›´æ–°å: å·²åˆ†é…: 0.77GB, å˜åŒ–: +0.38GB


é€šè¿‡è¿™ä¸ªåˆ†æå·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°åœ¨æ¯ä¸ªè®­ç»ƒé˜¶æ®µæ˜¾å­˜çš„ä½¿ç”¨æƒ…å†µå˜åŒ–ã€‚åœ¨å®é™…çš„å¤§æ¨¡å‹è®­ç»ƒä¸­ï¼Œè¿™äº›æ˜¾å­˜å ç”¨ä¼šæˆå€å¢é•¿ï¼Œå‡¸æ˜¾äº† ZeRO ä¼˜åŒ–çš„å¿…è¦æ€§ã€‚

```
åˆå§‹çŠ¶æ€: å·²åˆ†é…: 0.00GB, å˜åŒ–: +0.00GB
æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.13GB
ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.00GB
æ•°æ®åŠ è½½å: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.00GB
å‰å‘ä¼ æ’­å: å·²åˆ†é…: 0.14GB, å˜åŒ–: +0.01GB
åå‘ä¼ æ’­å: å·²åˆ†é…: 0.27GB, å˜åŒ–: +0.13GB
ä¼˜åŒ–å™¨æ›´æ–°å: å·²åˆ†é…: 0.52GB, å˜åŒ–: +0.25GB
```

## 2. ZeRO-1: ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡

ZeRO-1 é€šè¿‡å°†ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡åˆ°å¤šä¸ª GPU ä¸Šæ¥å‡å°‘æ˜¾å­˜å ç”¨ã€‚åœ¨ä¼ ç»Ÿæ•°æ®å¹¶è¡Œä¸­ï¼Œæ¯ä¸ª GPU éƒ½ä¿å­˜å®Œæ•´çš„ä¼˜åŒ–å™¨çŠ¶æ€å‰¯æœ¬ï¼Œè¿™é€ æˆäº†å¤§é‡çš„æ˜¾å­˜å†—ä½™ã€‚

ZeRO-1 çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šæ¯ä¸ª GPU åªä¿å­˜ä¸€éƒ¨åˆ†ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå½“éœ€è¦æ›´æ–°å‚æ•°æ—¶ï¼Œé€šè¿‡é›†åˆé€šä¿¡æ“ä½œè·å–å®Œæ•´çš„æ¢¯åº¦ä¿¡æ¯ã€‚

æ•°å­¦è¡¨è¾¾ä¸Šï¼Œå¯¹äº Adam ä¼˜åŒ–å™¨ï¼Œæ¯ä¸ª GPU åŸæœ¬éœ€è¦å­˜å‚¨ï¼š

- å‚æ•°ï¼š$Î˜$
- æ¢¯åº¦ï¼š$âˆ‡Î˜$
- åŠ¨é‡ï¼š$m$
- æ–¹å·®ï¼š$v$

ZeRO-1 åˆ†ç‰‡åï¼Œæ¯ä¸ª GPU åªå­˜å‚¨ï¼š

- å®Œæ•´å‚æ•°ï¼š$Î˜$
- å®Œæ•´æ¢¯åº¦ï¼š$âˆ‡Î˜$
- 1/N çš„åŠ¨é‡ï¼š$m_i$
- 1/N çš„æ–¹å·®ï¼š$v_i$

å…¶ä¸­ N æ˜¯ GPU æ•°é‡ã€‚

![](./images/Code01ZeRO01.png)

ZeRO-1 ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡


```python
class Zero1Optimizer:
    """ZeRO-1 æ¨¡æ‹Ÿå®ç°ï¼šå°†å‚æ•°åˆ†ç‰‡ï¼Œæ¯ä¸ªåˆ†ç‰‡ç‹¬ç«‹ä¼˜åŒ–å™¨ï¼ˆå• GPU æ¨¡æ‹Ÿï¼‰"""

    def __init__(self, params, optimizer_class=torch.optim.Adam, num_shards=4, **kwargs):
        self.params = list(params)
        self.num_shards = num_shards

        # å°†å‚æ•°å‡åŒ€åˆ†ç‰‡
        self.shards = []
        shard_size = (len(self.params) + num_shards - 1) // num_shards
        for i in range(0, len(self.params), shard_size):
            self.shards.append(self.params[i:i + shard_size])

        # ä¸ºæ¯ä¸ªåˆ†ç‰‡åˆ›å»ºç‹¬ç«‹ä¼˜åŒ–å™¨
        self.optimizers = [
            optimizer_class(shard, **kwargs) for shard in self.shards
        ]

    def zero_grad(self):
        for p in self.params:
            if p.grad is not None:
                p.grad.zero_()

    def step(self):
        # æ¯ä¸ªåˆ†ç‰‡ç‹¬ç«‹æ›´æ–°ï¼ˆæ¨¡æ‹Ÿå¤š GPU å„è‡ªæ›´æ–°è‡ªå·±çš„åˆ†ç‰‡ï¼‰
        for opt in self.optimizers:
            opt.step()

# æµ‹è¯• ZeRO-1 æ•ˆæœ
def test_zero1(seed=42):
    if not torch.cuda.is_available():
        return None

    torch.manual_seed(seed)
    torch.cuda.empty_cache()
    analyzer = MemoryAnalyzer()

    model = create_model().cuda()
    analyzer.record("æ¨¡å‹åˆ›å»ºå")

    optimizer = Zero1Optimizer(model.parameters(), num_shards=4, lr=1e-3)
    analyzer.record("ZeRO-1 ä¼˜åŒ–å™¨åˆ›å»ºå")

    inputs = torch.randn(32, 2048, device='cuda')
    outputs = model(inputs)
    loss = F.mse_loss(outputs, torch.randn_like(outputs))
    loss.backward()
    optimizer.step()
    analyzer.record("è®­ç»ƒä¸€æ­¥å")

    return analyzer.memory_stats

# æ‰§è¡Œæµ‹è¯•
zero1_stats = test_zero1()
```

    æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.20GB
    ZeRO-1 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB
    è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.77GB, å˜åŒ–: +0.56GB


è¿™ä¸ªç®€åŒ–å®ç°å±•ç¤ºäº† ZeRO-1 çš„æ ¸å¿ƒæ€æƒ³ï¼šæ¯ä¸ª GPU åªå­˜å‚¨å’Œæ›´æ–°ä¸€éƒ¨åˆ†å‚æ•°çš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œé€šè¿‡é€šä¿¡æ“ä½œç¡®ä¿æ‰€æœ‰ GPU çš„å‚æ•°ä¿æŒä¸€è‡´ã€‚

```
æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.14GB, å˜åŒ–: +0.14GB
ZeRO-1 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.14GB, å˜åŒ–: +0.00GB
è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.52GB, å˜åŒ–: +0.38GB
```

## 3. ZeRO-2: æ¢¯åº¦åˆ†ç‰‡

ZeRO-2 åœ¨ ZeRO-1 çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œä¸ä»…åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œè¿˜åˆ†ç‰‡æ¢¯åº¦ã€‚è¿™è¿›ä¸€æ­¥å‡å°‘äº†æ˜¾å­˜å ç”¨ï¼Œå› ä¸ºæ¢¯åº¦é€šå¸¸ä¸å‚æ•°å¤§å°ç›¸åŒã€‚

![](./images/Code01ZeRO02.png)

åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ª GPU è®¡ç®—å…¶åˆ†é…åˆ°çš„å‚æ•°çš„æ¢¯åº¦ï¼Œç„¶åé€šè¿‡ Reduce-Scatter æ“ä½œèšåˆæ¢¯åº¦ã€‚è¿™æ ·æ¯ä¸ª GPU åªä¿å­˜ä¸€éƒ¨åˆ†æ¢¯åº¦ï¼Œè€Œä¸æ˜¯å…¨éƒ¨æ¢¯åº¦ã€‚æ¢¯åº¦åˆ†ç‰‡çš„æ•°å­¦è¡¨è¾¾ï¼š

- ä¼ ç»Ÿæ–¹æ³•ï¼šæ¯ä¸ª GPU å­˜å‚¨å®Œæ•´æ¢¯åº¦ $âˆ‡Î˜$
- ZeRO-2ï¼šæ¯ä¸ª GPU å­˜å‚¨ 1/N çš„æ¢¯åº¦ $âˆ‡Î˜_i$


```python
class Zero2Optimizer(Zero1Optimizer):
    """ZeRO-2 æ¨¡æ‹Ÿï¼šåœ¨ ZeRO-1 åŸºç¡€ä¸Šï¼Œåªä¿ç•™å½“å‰åˆ†ç‰‡çš„æ¢¯åº¦"""

    def step(self):
        current_shard_idx = 0  # å‡è®¾å½“å‰ GPU è´Ÿè´£ç¬¬ 0 ä¸ªåˆ†ç‰‡

        # åˆ é™¤éæœ¬åˆ†ç‰‡çš„æ¢¯åº¦ï¼ˆæ¨¡æ‹Ÿ reduce-scatter åé‡Šæ”¾ï¼‰
        for shard_idx, shard in enumerate(self.shards):
            for param in shard:
                if param.grad is not None:
                    if shard_idx != current_shard_idx:
                        param.grad = None  # âœ… é‡Šæ”¾æ¢¯åº¦æ˜¾å­˜

        # ä»…æ›´æ–°æœ¬åˆ†ç‰‡
        self.optimizers[current_shard_idx].step()

# æµ‹è¯• ZeRO-2 æ•ˆæœ
def test_zero2(seed=42):
    if not torch.cuda.is_available():
        return None

    torch.manual_seed(seed)
    torch.cuda.empty_cache()
    analyzer = MemoryAnalyzer()

    model = create_model().cuda()
    analyzer.record("æ¨¡å‹åˆ›å»ºå")

    optimizer = Zero2Optimizer(model.parameters(), num_shards=4, lr=1e-3)
    analyzer.record("ZeRO-2 ä¼˜åŒ–å™¨åˆ›å»ºå")

    inputs = torch.randn(32, 2048, device='cuda')
    outputs = model(inputs)
    loss = F.mse_loss(outputs, torch.randn_like(outputs))
    loss.backward()
    optimizer.step()
    analyzer.record("è®­ç»ƒä¸€æ­¥å")

    return analyzer.memory_stats

zero2_stats = test_zero2()
```

    æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.20GB
    ZeRO-2 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB
    è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.34GB, å˜åŒ–: +0.14GB


ZeRO-2 é€šè¿‡æ¢¯åº¦åˆ†ç‰‡è¿›ä¸€æ­¥å‡å°‘äº†æ˜¾å­˜å ç”¨ï¼Œä½†å¢åŠ äº†é€šä¿¡å¼€é”€ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦æ ¹æ®ç½‘ç»œå¸¦å®½å’Œè®¡ç®—èƒ½åŠ›æƒè¡¡è¿™ç§æƒè¡¡ã€‚

```
æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.13GB
ZeRO-2 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.00GB
è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.31GB, å˜åŒ–: +0.18GB
```

## 4. ZeRO-3: å‚æ•°åˆ†ç‰‡

ZeRO-3 æ˜¯ ZeRO ç³»åˆ—çš„æœ€ç»ˆå½¢æ€ï¼Œå®ƒä¸ä»…åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼Œè¿˜åˆ†ç‰‡æ¨¡å‹å‚æ•°æœ¬èº«ã€‚è¿™æ„å‘³ç€æ¯ä¸ª GPU åªå­˜å‚¨æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œå¤§å¤§é™ä½äº†å•ä¸ª GPU çš„æ˜¾å­˜éœ€æ±‚ã€‚

![](./images/Code01ZeRO03.png)

ZeRO-3 çš„å·¥ä½œåŸç†ï¼š

1. å‰å‘ä¼ æ’­æ—¶ï¼Œæ¯ä¸ª GPU åªè®¡ç®—å®ƒæ‹¥æœ‰çš„å‚æ•°éƒ¨åˆ†
2. éœ€è¦å…¶ä»– GPU çš„å‚æ•°æ—¶ï¼Œé€šè¿‡é€šä¿¡æ“ä½œè·å–
3. åå‘ä¼ æ’­æ—¶ç±»ä¼¼ï¼Œåªè®¡ç®—æœ¬åœ°å‚æ•°çš„æ¢¯åº¦
4. é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„é€šä¿¡æ¨¡å¼æœ€å°åŒ–é€šä¿¡å¼€é”€

å‚æ•°åˆ†ç‰‡çš„æ•°å­¦è¡¨è¾¾ï¼š

- ä¼ ç»Ÿæ–¹æ³•ï¼šæ¯ä¸ª GPU å­˜å‚¨å®Œæ•´å‚æ•° $Î˜$
- ZeRO-3ï¼šæ¯ä¸ª GPU å­˜å‚¨ 1/N çš„å‚æ•° $Î˜_i$


```python
class Zero3Model(nn.Module):
    """ZeRO-3 æ¨¡æ‹Ÿï¼šä»…åŠ è½½æ¨¡å‹çš„ä¸€éƒ¨åˆ†å±‚ï¼ˆå‚æ•°åˆ†ç‰‡ï¼‰"""

    def __init__(self, base_model, shard_id=0, num_shards=4):
        super().__init__()
        self.shard_id = shard_id
        self.num_shards = num_shards

        # è®¡ç®—å½“å‰åˆ†ç‰‡è´Ÿè´£çš„å±‚èŒƒå›´
        total_layers = len(base_model)
        layers_per_shard = (total_layers + num_shards - 1) // num_shards
        start = shard_id * layers_per_shard
        end = min(start + layers_per_shard, total_layers)

        # ä»…ä¿ç•™æœ¬åˆ†ç‰‡çš„å±‚
        self.layers = nn.ModuleList([base_model[i] for i in range(start, end)])

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

# æµ‹è¯• ZeRO-3 æ•ˆæœ
def test_zero3(seed=42):
    if not torch.cuda.is_available():
        return None

    torch.manual_seed(seed)
    torch.cuda.empty_cache()
    analyzer = MemoryAnalyzer()

    base_model = create_model()
    model = Zero3Model(base_model, shard_id=0, num_shards=4).cuda()
    analyzer.record("ZeRO-3 æ¨¡å‹åˆ›å»ºå")

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    analyzer.record("ä¼˜åŒ–å™¨åˆ›å»ºå")

    inputs = torch.randn(32, 2048, device='cuda')
    outputs = model(inputs)
    loss = F.mse_loss(outputs, torch.randn_like(outputs))
    loss.backward()
    optimizer.step()
    analyzer.record("è®­ç»ƒä¸€æ­¥å")

    return analyzer.memory_stats

# æ‰§è¡Œæµ‹è¯•
zero3_stats = test_zero3()
```

    ZeRO-3 æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.06GB, å˜åŒ–: +0.06GB
    ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.06GB, å˜åŒ–: +0.00GB
    è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.14GB


ZeRO-3 æä¾›äº†æœ€å¤§çš„æ˜¾å­˜èŠ‚çœï¼Œä½†é€šä¿¡å¼€é”€ä¹Ÿæœ€å¤§ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€šå¸¸éœ€è¦ç»“åˆå„ç§ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚é€šä¿¡è®¡ç®—é‡å ã€æ¢¯åº¦ç´¯ç§¯ç­‰ï¼Œæ¥å¹³è¡¡æ˜¾å­˜èŠ‚çœå’Œè®­ç»ƒé€Ÿåº¦ã€‚

```
ZeRO-3 æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.03GB, å˜åŒ–: +0.03GB
ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.03GB, å˜åŒ–: +0.00GB
è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.11GB, å˜åŒ–: +0.08GB
```

## 5. Zero Offload æŠ€æœ¯

Zero Offload æŠ€æœ¯å°†ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°å¸è½½åˆ° CPU å†…å­˜æˆ– NVMe å­˜å‚¨ï¼Œè¿›ä¸€æ­¥æ‰©å±•äº†å¯è®­ç»ƒçš„æ¨¡å‹è§„æ¨¡ã€‚è¿™ç§æŠ€æœ¯ç‰¹åˆ«é€‚åˆåœ¨æœ‰é™ GPU å†…å­˜ç¯å¢ƒä¸‹è®­ç»ƒè¶…å¤§æ¨¡å‹ã€‚

![](./images/Code01ZeRO04.png)

Offload çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ CPU å†…å­˜å’Œ NVMe å­˜å‚¨ä½œä¸º GPU æ˜¾å­˜çš„æ‰©å±•ï¼Œé€šè¿‡å¼‚æ­¥æ•°æ®ä¼ è¾“å’Œè®¡ç®—é‡å æ¥æœ€å°åŒ–æ€§èƒ½å½±å“ã€‚


```python
class CPUOffloadOptimizer:
    """CPU Offload æ¨¡æ‹Ÿï¼šä¼˜åŒ–å™¨çŠ¶æ€å­˜å‚¨åœ¨ CPU"""

    def __init__(self, params, optimizer_class=torch.optim.Adam, **kwargs):
        self.gpu_params = list(params)
        # åœ¨ CPU ä¸Šåˆ›å»ºå‚æ•°å‰¯æœ¬ï¼ˆæ— æ¢¯åº¦ï¼‰
        self.cpu_params = [p.detach().cpu().clone() for p in self.gpu_params]
        self.optimizer = optimizer_class(self.cpu_params, **kwargs)

    def step(self):
        # æ¢¯åº¦ä» GPU â†’ CPU
        for gpu_p, cpu_p in zip(self.gpu_params, self.cpu_params):
            if gpu_p.grad is not None:
                cpu_p.grad = gpu_p.grad.cpu()

        # åœ¨ CPU ä¸Šæ›´æ–°
        self.optimizer.step()

        # å‚æ•°ä» CPU â†’ GPU
        for gpu_p, cpu_p in zip(self.gpu_params, self.cpu_params):
            gpu_p.data.copy_(cpu_p.data)

        # æ¸…ç† CPU æ¢¯åº¦
        for cpu_p in self.cpu_params:
            cpu_p.grad = None

# æµ‹è¯• CPU Offload æ•ˆæœ
def test_cpu_offload(seed=42):
    if not torch.cuda.is_available():
        return None

    torch.manual_seed(seed)
    torch.cuda.empty_cache()
    analyzer = MemoryAnalyzer()

    model = create_model().cuda()
    analyzer.record("æ¨¡å‹åˆ›å»ºå")

    optimizer = CPUOffloadOptimizer(model.parameters(), lr=1e-3)
    analyzer.record("CPU Offload ä¼˜åŒ–å™¨åˆ›å»ºå")

    inputs = torch.randn(32, 2048, device='cuda')
    outputs = model(inputs)
    loss = F.mse_loss(outputs, torch.randn_like(outputs))
    loss.backward()
    optimizer.step()
    analyzer.record("è®­ç»ƒä¸€æ­¥å")

    return analyzer.memory_stats

# æ‰§è¡Œæµ‹è¯•
offload_stats = test_cpu_offload()
```

    æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.20GB
    CPU Offload ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB
    è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.39GB, å˜åŒ–: +0.19GB


```
æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.13GB
CPU Offload ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.13GB, å˜åŒ–: +0.00GB
è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.25GB, å˜åŒ–: +0.12GB
```

## 6. æ€§èƒ½åˆ†æä¸å®éªŒç»“æœ

ä¸ºäº†éªŒè¯ ZeRO å„çº§åˆ«çš„æ•ˆæœï¼Œæˆ‘ä»¬è®¾è®¡äº†ä»¥ä¸‹å®éªŒï¼š


```python
# æ±‡æ€»æ‰€æœ‰æ–¹æ³•çš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
def compare_methods():
    if not torch.cuda.is_available():
        return

    print("\n æ˜¾å­˜ä½¿ç”¨å¯¹æ¯” (å•ä½: GB):")
    print("-" * 40)

    # é‡æ–°è¿è¡ŒåŸºç¡€æµ‹è¯•
    baseline = analyze_memory()
    print("-" * 40)
    zero1 = test_zero1()
    print("-" * 40)
    zero2 = test_zero2()
    print("-" * 40)
    zero3 = test_zero3()
    print("-" * 40)
    offload = test_cpu_offload()
    print("-" * 40)

    # æå–æœ€ç»ˆæ˜¾å­˜ä½¿ç”¨é‡
    print(f"åŸºç¡€æ–¹æ³•: {baseline['allocated'][-1]:.2f}GB")
    print(f"ZeRO-1: {zero1['allocated'][-1]:.2f}GB ({(1-zero1['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% èŠ‚çœ)")
    print(f"ZeRO-2: {zero2['allocated'][-1]:.2f}GB ({(1-zero2['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% èŠ‚çœ)")
    print(f"ZeRO-3: {zero3['allocated'][-1]:.2f}GB ({(1-zero3['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% èŠ‚çœ)")
    print(f"CPU Offload: {offload['allocated'][-1]:.2f}GB ({(1-offload['allocated'][-1]/baseline['allocated'][-1])*100:.1f}% èŠ‚çœ)")

# æ‰§è¡Œå¯¹æ¯”
compare_methods()
```

    
     æ˜¾å­˜ä½¿ç”¨å¯¹æ¯” (å•ä½: GB):
    ----------------------------------------
    åˆå§‹çŠ¶æ€: å·²åˆ†é…: 0.02GB, å˜åŒ–: +0.02GB
    æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.19GB
    ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB
    æ•°æ®åŠ è½½å: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB
    å‰å‘ä¼ æ’­å: å·²åˆ†é…: 0.21GB, å˜åŒ–: +0.00GB
    åå‘ä¼ æ’­å: å·²åˆ†é…: 0.39GB, å˜åŒ–: +0.18GB
    ä¼˜åŒ–å™¨æ›´æ–°å: å·²åˆ†é…: 0.77GB, å˜åŒ–: +0.38GB
    ----------------------------------------
    æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.20GB
    ZeRO-1 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB
    è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.77GB, å˜åŒ–: +0.56GB
    ----------------------------------------
    æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.20GB
    ZeRO-2 ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB
    è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.34GB, å˜åŒ–: +0.14GB
    ----------------------------------------
    ZeRO-3 æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.06GB, å˜åŒ–: +0.06GB
    ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.06GB, å˜åŒ–: +0.00GB
    è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.14GB
    ----------------------------------------
    æ¨¡å‹åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.20GB
    CPU Offload ä¼˜åŒ–å™¨åˆ›å»ºå: å·²åˆ†é…: 0.20GB, å˜åŒ–: +0.00GB
    è®­ç»ƒä¸€æ­¥å: å·²åˆ†é…: 0.39GB, å˜åŒ–: +0.19GB
    ----------------------------------------
    åŸºç¡€æ–¹æ³•: 0.77GB
    ZeRO-1: 0.77GB (0.0% èŠ‚çœ)
    ZeRO-2: 0.34GB (55.0% èŠ‚çœ)
    ZeRO-3: 0.20GB (73.4% èŠ‚çœ)
    CPU Offload: 0.39GB (48.9% èŠ‚çœ)


é€šè¿‡è¿™ä¸ªå®éªŒï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ° ZeRO å„çº§åˆ«å¯¹æ˜¾å­˜å ç”¨çš„ä¼˜åŒ–æ•ˆæœã€‚åœ¨å®é™…çš„å¤§æ¨¡å‹è®­ç»ƒä¸­ï¼Œè¿™äº›ä¼˜åŒ–å¯ä»¥å¸¦æ¥æ•°å€ç”šè‡³æ•°åå€çš„æ˜¾å­˜èŠ‚çœã€‚

```
æ˜¾å­˜ä½¿ç”¨å¯¹æ¯” (å•ä½: GB):
----------------------------------------
åŸºç¡€æ–¹æ³•: 0.39GB
ZeRO-1: 0.39GB (0.0% èŠ‚çœ)
ZeRO-2: 0.31GB (20.5% èŠ‚çœ)
ZeRO-3: 0.11GB (71.8% èŠ‚çœ)
CPU Offload: 0.25GB (35.9% èŠ‚çœ)
```

## æ€»ç»“ä¸æ€è€ƒ

ZeRO æŠ€æœ¯é€šè¿‡åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ï¼Œæ˜¾è‘—é™ä½äº†å¤§æ¨¡å‹è®­ç»ƒçš„æ˜¾å­˜éœ€æ±‚ã€‚æœ¬å®éªŒé€šè¿‡ä»£ç å®ç°å’ŒåŸç†åˆ†æï¼Œæ·±å…¥æ¢è®¨äº†ï¼š

1. **ZeRO-1**ï¼šä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ï¼Œå‡å°‘çº¦ 4 å€æ˜¾å­˜å ç”¨
2. **ZeRO-2**ï¼šæ¢¯åº¦åˆ†ç‰‡ï¼Œè¿›ä¸€æ­¥å‡å°‘çº¦ 8 å€æ˜¾å­˜å ç”¨  
3. **ZeRO-3**ï¼šå‚æ•°åˆ†ç‰‡ï¼Œæœ€å¤§å¯å‡å°‘çº¦ N å€æ˜¾å­˜å ç”¨ï¼ˆN ä¸º GPU æ•°é‡ï¼‰
4. **Zero Offload**ï¼šå°†æ•°æ®å¸è½½åˆ° CPU/NVMeï¼Œæ”¯æŒè®­ç»ƒè¶…å¤§æ¨¡å‹

è¿™äº›æŠ€æœ¯å¯ä»¥ç»„åˆä½¿ç”¨ï¼Œæ ¹æ®å…·ä½“çš„ç¡¬ä»¶ç¯å¢ƒå’Œæ¨¡å‹å¤§å°é€‰æ‹©æœ€åˆé€‚çš„é…ç½®ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒDeepSpeed æ¡†æ¶æä¾›äº†å®Œæ•´çš„ ZeRO å®ç°ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨ç»è¿‡ä¼˜åŒ–çš„å®˜æ–¹å®ç°ã€‚

## å¼•ç”¨ä¸å‚è€ƒ

- https://arxiv.org/abs/1910.02054
- https://www.cnblogs.com/whiteBear/p/18341975
